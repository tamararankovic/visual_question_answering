{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "28595c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Layer, Concatenate, Input, Dense\n",
    "import keras.backend as K\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "48f69028",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'model'\n",
    "tanh_layer_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "57a45d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "2133c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGAttention(Layer):\n",
    "    def __init__(self, tanh_layer_size):\n",
    "        super(QGAttention, self).__init__()\n",
    "        self.tanh_layer_size = tanh_layer_size\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W1 = self.add_weight(name='attention_weight', shape=(input_shape[2], self.tanh_layer_size), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.W2 = self.add_weight(name='attention_weight2', shape=(self.tanh_layer_size, 1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        #self.b=self.add_weight(name='attention_bias', shape=(1, 32), \n",
    "        #                       initializer='zeros', trainable=True)        \n",
    "        super(QGAttention, self).build(input_shape)\n",
    " \n",
    "    # x = [batch_size, vector_length, regions]\n",
    "    def call(self,x):\n",
    "        #tanh layer\n",
    "        e = K.tanh(K.dot(x, self.W1))\n",
    "        #linear layer\n",
    "        #print('e: ', e)\n",
    "        e = K.dot(e, self.W2)\n",
    "        #print('e: ', e)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        #print('e: ', e)\n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        #print('alpha: ', alpha)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        #print('alpha: ', alpha)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        #print('context: ', context)\n",
    "        context = K.sum(context, axis=1)\n",
    "        #print('context: ', context)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "cd32a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(vector_input_size, tanh_layer_size):\n",
    "    inputs = Input(shape=(None, vector_input_size))\n",
    "    inputs2 = QGAttention(tanh_layer_size)(inputs)\n",
    "    outputs = Dense(1)(inputs2)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "003945f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y):\n",
    "    model = get_model(x.shape[2], tanh_layer_size)\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(x, y, verbose=2)\n",
    "    model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "e1d2a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    model = load_model(model_dir)\n",
    "    print(model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "a4406e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_61\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_200 (InputLayer)      [(None, None, 1280)]      0         \n",
      "                                                                 \n",
      " qg_attention_11 (QGAttentio  (None, 1280)             384300    \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 385,581\n",
      "Trainable params: 385,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "1/1 - 0s - loss: 1.0127 - 15ms/epoch - 15ms/step\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "#jedan element x-a je lista konkateniranih vektora regiona i odgovarajuceg pitanja\n",
    "x = np.random.random((2, 5, 1280))\n",
    "#klasa\n",
    "y = np.random.random((2, 1))\n",
    "\n",
    "train(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "e57af7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "[[ 0.4374066 ]\n",
      " [-0.04947354]]\n"
     ]
    }
   ],
   "source": [
    "predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3b6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
